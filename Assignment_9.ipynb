{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn0JedBRzg02R8ia3PsLB+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupriyaSingh1997/ML_Assignment/blob/main/Assignment_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
        "\n",
        "Feature engineering is the process of transforming raw data into a set of meaningful features that can be used to train a machine learning model. It involves selecting, creating, and transforming features to improve the performance of the model.\n",
        "\n",
        "Feature engineering is a crucial step in the machine learning pipeline because the quality and relevance of the features greatly impact the model's ability to learn and make accurate predictions. Here are the various aspects of feature engineering:\n",
        "\n",
        "Feature Selection:\n",
        "\n",
        "Univariate Selection: In this approach, each feature is evaluated independently and ranked based on statistical tests like chi-square, ANOVA, or correlation with the target variable. The top-ranked features are selected for the model.\n",
        "Model-based Selection: A machine learning model is trained on the features, and their importance is derived from the model's coefficients, feature importance scores, or similar techniques.\n",
        "\n",
        "Recursive Feature Elimination: This method involves recursively eliminating features based on their importance until the optimal subset of features is obtained.\n",
        "Feature Creation:\n",
        "\n",
        "Polynomial Features: New features are created by combining existing features through mathematical operations such as multiplication, addition, or exponentiation. This allows the model to capture non-linear relationships.\n",
        "Interaction Features: Interactions between existing features are created to capture combined effects. For example, in a housing dataset, multiplying the number of bedrooms and bathrooms may provide a feature representing the total number of functional rooms.\n",
        "Domain-specific Features: Domain knowledge can be leveraged to create features that capture specific patterns or relationships in the data. For instance, in natural language processing, features like word frequency, n-grams, or sentiment scores can be created.\n",
        "Feature Transformation:\n",
        "\n",
        "Scaling: Features are scaled to a similar range to prevent certain features from dominating others. Common scaling techniques include standardization (mean of 0 and standard deviation of 1) or normalization (scaling to a range between 0 and 1).\n",
        "Logarithmic Transformation: Taking the logarithm of skewed features can help\n",
        "normalize their distribution and make them more suitable for certain models.\n",
        "Binning: Continuous features can be converted into categorical features by dividing them into bins or intervals. This can capture non-linear relationships and handle outliers.\n",
        "Handling Missing Data:\n",
        "\n",
        "Imputation: Missing values can be filled in using techniques like mean, median, mode, or using more advanced methods such as regression imputation or K-nearest neighbors imputation.\n",
        "Creating Indicator Variables: A binary indicator feature can be created to indicate whether a value was missing or not, which can be useful if the missingness itself carries information.\n",
        "Encoding Categorical Variables:\n",
        "\n",
        "One-Hot Encoding: Each category is represented by a binary feature (0 or 1), where only one category is active at a time. This is suitable for algorithms that do not assume any order or hierarchy among categories.\n",
        "Ordinal Encoding: Categories are assigned numerical values based on a specific order or ranking. This is suitable for categorical variables with an inherent order, such as ratings (low, medium, high)."
      ],
      "metadata": {
        "id": "1euCfKtjDY9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
        "\n",
        "Feature selection is the process of selecting a subset of relevant features from the original set of features to improve the performance of a machine learning model. The aim of feature selection is to reduce the dimensionality of the data, remove irrelevant or redundant features, and focus on the most informative features for the prediction task at hand. This can lead to simpler and more interpretable models, faster training times, and better generalization.\n",
        "\n",
        "There are various methods of feature selection, including:\n",
        "\n",
        "Univariate Selection:\n",
        "\n",
        "Statistical tests: Features are evaluated independently using statistical tests such as chi-square for categorical variables, ANOVA for numerical variables, or correlation with the target variable. Features with high test scores or correlation coefficients are considered relevant and selected.\n",
        "SelectKBest: This method selects the top K features based on statistical tests.\n",
        "\n",
        "Model-based Selection:\n",
        "\n",
        "Recursive Feature Elimination (RFE): This method involves training a model on the full set of features, ranking the features based on their importance, and recursively eliminating the least important features until the desired number of features is reached.\n",
        "L1 Regularization (Lasso): L1 regularization adds a penalty term to the loss function of a model, forcing some of the model's coefficients (corresponding to features) to become zero. Features with non-zero coefficients are selected.\n",
        "Dimensionality Reduction Techniques:\n",
        "\n",
        "Principal Component Analysis (PCA): PCA transforms the original features into a new set of orthogonal features called principal components. These components capture the maximum variance in the data. Selecting the top N components can be an indirect way of selecting features.\n",
        "Linear Discriminant Analysis (LDA): LDA is a supervised dimensionality reduction technique that aims to find linear combinations of features that maximize class separability. It can be used for feature selection in classification tasks.\n",
        "Feature Importance:\n",
        "\n",
        "Tree-based models: Decision trees, random forests, and gradient boosting models\n",
        "provide feature importance scores based on how often a feature is used to make splits and their impact on the overall performance. Features with higher importance scores are considered more relevant.\n",
        "Coefficient Magnitude: In linear models like linear regression or logistic regression, the magnitude of the coefficients can indicate the importance of the corresponding features. Features with larger coefficients are given more weight in the model."
      ],
      "metadata": {
        "id": "AUgLt_nDDY59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\n",
        "\n",
        "i. Describe the overall feature selection process.\n",
        "\n",
        "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
        "widely used function extraction algorithms?\n",
        "\n",
        "\n",
        "i. The overall feature selection process typically involves the following steps:\n",
        "\n",
        "Data Preparation: Preprocess and clean the dataset, handle missing values, and encode categorical variables if necessary.\n",
        "\n",
        "Feature Generation: Create new features by combining or transforming existing features using techniques like polynomial features, interaction features, or domain-specific features.\n",
        "\n",
        "Feature Selection: Select a subset of relevant features from the generated feature set. This can be done using methods such as univariate selection, model-based selection, dimensionality reduction techniques, or feature importance.\n",
        "\n",
        "Model Training and Evaluation: Train a machine learning model using the selected features and evaluate its performance using appropriate metrics. This step helps determine whether the selected features contribute positively to the model's predictive power.\n",
        "\n",
        "Iterative Refinement: If the model performance is not satisfactory, iterate through the feature engineering process by adjusting the feature selection criteria, creating new features, or trying different models until the desired performance is achieved.\n",
        "\n",
        "ii. The key underlying principle of feature extraction is to transform the original features into a new set of features that capture the most relevant information for the prediction task. These new features are often derived through mathematical or statistical techniques and aim to represent the underlying patterns or structures in the data.\n",
        "\n",
        "An example of feature extraction is using the Discrete Fourier Transform (DFT) to extract frequency domain features from a time series signal. Let's say we have a dataset consisting of accelerometer readings capturing human activities such as walking, running, and sitting. The raw accelerometer data represents the acceleration values over time.\n",
        "\n",
        "The most widely used feature extraction algorithms include:\n",
        "\n",
        "Principal Component Analysis (PCA): PCA transforms the original features into a new set of orthogonal features called principal components. These components capture the maximum variance in the data and can be used as reduced-dimensional representations.\n",
        "\n",
        "Independent Component Analysis (ICA): ICA is a statistical technique that separates a multivariate signal into its independent components by assuming that the observed signal is a linear combination of unknown independent source signals.\n",
        "\n",
        "Linear Discriminant Analysis (LDA): LDA is a dimensionality reduction technique that aims to find linear combinations of features that maximize class separability. It is widely used in classification tasks.\n",
        "\n",
        "Wavelet Transform: The wavelet transform decomposes a signal into different frequency components at different scales, capturing both time and frequency information. It is commonly used in signal processing and image analysis tasks.\n",
        "\n",
        "These are just a few examples of feature extraction algorithms, and the choice of algorithm depends on the specific characteristics of the data and the problem at hand."
      ],
      "metadata": {
        "id": "ncUbyiefDY3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Describe the feature engineering process in the sense of a text categorization issue.\n",
        "\n",
        "The feature engineering process in the context of text categorization involves transforming raw text data into a set of meaningful features that can be used to train a machine learning model for classifying or categorizing text documents. Here's a step-by-step description of the feature engineering process for text categorization:\n",
        "\n",
        "Text Preprocessing:\n",
        "\n",
        "Tokenization: Split the text into individual words or tokens.\n",
        "Lowercasing: Convert all tokens to lowercase to ensure case insensitivity.\n",
        "Removing Punctuation: Remove punctuation marks that don't carry significant meaning.\n",
        "Stopword Removal: Remove common words like \"the,\" \"is,\" \"and\" that do not contribute much to the meaning of the text.\n",
        "Stemming or Lemmatization: Reduce words to their base or root form to handle different forms of the same word (e.g., \"running\" and \"runs\" to \"run\").\n",
        "Feature Extraction:\n",
        "\n",
        "Bag-of-Words (BoW) Representation: Create a vocabulary of unique words from the preprocessed text and represent each document as a vector where each element\n",
        "corresponds to the count or presence of a word in the vocabulary.\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): Calculate the importance of each word in a document by considering its frequency in the document and inversely proportional to its frequency across all documents in the corpus.\n",
        "N-grams: Instead of considering only individual words, include sequences of adjacent words (bi-grams, tri-grams, etc.) as features to capture more contextual information.\n",
        "Word Embeddings: Utilize pre-trained word embeddings like Word2Vec, GloVe, or fastText, which represent words as dense vectors capturing semantic relationships. These embeddings can be used as features or as a basis for deriving sentence or document-level representations.\n",
        "Feature Selection:\n",
        "\n",
        "Remove Low-Information Features: Exclude features that have low variance or occur in a very small number of documents, as they may not contribute much discriminatory power.\n",
        "Select Top-K Features: Choose the most informative features based on statistical measures like chi-square, information gain, or mutual information score, which capture the relevance between features and the target categories.\n",
        "\n",
        "Additional Text-specific Features:\n",
        "\n",
        "Document Length: Include the length of the document as a feature, as document length can sometimes correlate with certain categories or writing styles.\n",
        "Part-of-Speech Tags: Extract the part-of-speech tags (e.g., noun, verb, adjective) of the words in the text to capture syntactic information.\n",
        "Named Entity Recognition: Identify and include named entities such as people, organizations, locations, or dates, which can be indicative of specific categories.\n",
        "Feature Transformation:\n",
        "\n",
        "Scaling: Normalize the feature vectors to a similar range to prevent certain features from dominating others.\n",
        "Dimensionality Reduction: Apply techniques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) to reduce the dimensionality of the feature space while retaining most of the information.\n",
        "Model Training and Evaluation:\n",
        "\n",
        "Use the preprocessed and transformed features along with the corresponding target labels to train a text classification model (e.g., Naive Bayes, Logistic Regression, Support Vector Machines, or deep learning models like Convolutional Neural Networks or Transformer-based models).\n",
        "Evaluate the model's performance using appropriate metrics such as accuracy, precision, recall, or F1 score."
      ],
      "metadata": {
        "id": "ePovROp7DY1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
        "cosine.\n",
        "\n",
        "Cosine similarity is a popular metric for text categorization due to its ability to measure the similarity between two text documents regardless of their length or magnitude. Here are a few reasons why cosine similarity is suitable for text categorization:\n",
        "\n",
        "Independence from Document Length: Cosine similarity is unaffected by the length of the documents being compared. It only considers the direction of the vectors, i.e., the angle between them, rather than their magnitude. This makes it robust when comparing documents of varying lengths.\n",
        "\n",
        "Focus on Term Frequency: Cosine similarity takes into account the frequency of terms in the document-term matrix. It captures the similarity based on how many terms the documents have in common and how frequently they occur, which is important for text categorization tasks.\n",
        "\n",
        "Robust to Sparse Representations: In text categorization, the document-term matrix is often sparse because most documents contain only a subset of the available vocabulary. Cosine similarity handles this sparsity well, as it only considers the non-zero elements in the vectors, effectively ignoring the missing terms.\n",
        "\n",
        "Now, let's calculate the cosine similarity for the given document-term matrix vectors:\n",
        "\n",
        "Document 1: (2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
        "Document 2: (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
        "\n",
        "To calculate the cosine similarity, we need to compute the dot product of the two vectors and divide it by the product of their magnitudes.\n",
        "\n",
        "Dot Product: (2 * 2) + (3 * 1) + (2 * 0) + (0 * 0) + (2 * 3) + (3 * 2) + (3 * 1) + (0 * 3) + (1 * 1) = 4 + 3 + 0 + 0 + 6 + 6 + 3 + 0 + 1 = 23\n",
        "\n",
        "Magnitude of Document 1: sqrt((2^2) + (3^2) + (2^2) + (0^2) + (2^2) + (3^2) + (3^2) + (0^2) + (1^2)) = sqrt(4 + 9 + 4 + 0 + 4 + 9 + 9 + 0 + 1) = sqrt(40) = 6.32\n",
        "\n",
        "Magnitude of Document 2: sqrt((2^2) + (1^2) + (0^2) + (0^2) + (3^2) + (2^2) + (1^2) + (3^2) + (1^2)) = sqrt(4 + 1 + 0 + 0 + 9 + 4 + 1 + 9 + 1) = sqrt(29) = 5.39\n",
        "\n",
        "Cosine Similarity = Dot Product / (Magnitude of Document 1 * Magnitude of Document 2) = 23 / (6.32 * 5.39) = 23 / 34.01 ≈ 0.6766\n",
        "\n",
        "Therefore, the cosine similarity between the two document vectors is approximately 0.6766."
      ],
      "metadata": {
        "id": "tkBmK9M5DYyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\n",
        "\n",
        "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
        "calculate the Hamming gap.\n",
        "\n",
        "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
        "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
        "\n",
        "i. The Hamming distance is a metric used to measure the difference between two strings of equal length. It calculates the number of positions at which the corresponding elements of the two strings are different. The formula for calculating Hamming distance is as follows:\n",
        "\n",
        "Hamming distance = number of positions with different elements\n",
        "\n",
        "Let's calculate the Hamming distance between 10001011 and 11001111:\n",
        "\n",
        "The positions with different elements are at the 4th and 5th positions. Therefore, the Hamming distance is 2.\n",
        "\n",
        "ii. The Jaccard index and similarity matching coefficient are two similarity measures used to compare sets or binary features.\n",
        "\n",
        "Jaccard Index:\n",
        "The Jaccard index is calculated as the size of the intersection of two sets divided by the size of their union. In the context of binary features, it represents the proportion of common \"1\" values between the two features.\n",
        "\n",
        "Set A: (1, 1, 0, 0, 1, 0, 1, 1)\n",
        "Set B: (1, 1, 0, 0, 0, 1, 1, 1)\n",
        "\n",
        "Intersection: (1, 1, 0, 0, 0, 0, 1, 1) (common \"1\" values)\n",
        "Union: (1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1) (all unique values)\n",
        "\n",
        "Jaccard Index = size of intersection / size of union = 8 / 11 ≈ 0.727\n",
        "\n",
        "Similarity Matching Coefficient:\n",
        "The similarity matching coefficient is calculated as the number of matches between corresponding elements of two sets divided by the total number of elements. In the context of binary features, it represents the proportion of matching values between the two features.\n",
        "\n",
        "Set A: (1, 1, 0, 0, 1, 0, 1, 1)\n",
        "Set B: (1, 0, 0, 1, 1, 0, 0, 1)\n",
        "\n",
        "Matches: (1, 0, 0, 0, 1, 0, 0, 1)\n",
        "\n",
        "Similarity Matching Coefficient = number of matches / total number of elements = 5 / 8 = 0.625\n",
        "\n",
        "Therefore, the Jaccard index is approximately 0.727, and the similarity matching coefficient is 0.625 for the given feature sets.\n",
        "\n",
        "10001011\n",
        "11001111"
      ],
      "metadata": {
        "id": "tOzGCpVbDYwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. State what is meant by high-dimensional data set? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
        "\n",
        "A high-dimensional dataset refers to a dataset where the number of features or dimensions is significantly large compared to the number of observations or samples. In other words, the dataset has a large number of variables or attributes that describe each data point.\n",
        "\n",
        "Real-life examples of high-dimensional datasets include:\n",
        "\n",
        "Genomic Data: DNA sequencing data with thousands or millions of genetic markers or genes.\n",
        "\n",
        "Image Data: High-resolution images with pixels or image features representing each pixel, resulting in a large number of dimensions.\n",
        "\n",
        "Text Data: Text documents represented using bag-of-words or TF-IDF features, where the number of unique words can lead to high-dimensional feature spaces.\n",
        "\n",
        "Sensor Data: Data collected from sensors in Internet of Things (IoT) applications, such as environmental monitoring or industrial systems, where numerous sensor readings contribute to high-dimensional datasets.\n",
        "\n",
        "Using machine learning techniques on high-dimensional datasets poses several challenges:\n",
        "\n",
        "Curse of Dimensionality: As the number of dimensions increases, the data becomes more sparse, making it harder to find meaningful patterns or relationships. This can lead to overfitting, where the model becomes too complex and fails to generalize well to unseen data.\n",
        "\n",
        "Increased Computational Complexity: Training machine learning models becomes computationally expensive as the number of dimensions grows. Many algorithms suffer from the \"curse of dimensionality\" as the computational complexity scales exponentially with the number of dimensions.\n",
        "\n",
        "Increased Risk of Noise: In high-dimensional spaces, there is an increased likelihood of noise or irrelevant features that can negatively impact the model's performance.\n",
        "\n",
        "To address these difficulties, several techniques can be employed:\n",
        "\n",
        "Feature Selection: Identify the most relevant features that contribute the most to the target variable and discard irrelevant or redundant features. This reduces dimensionality and improves model efficiency and interpretability.\n",
        "\n",
        "Dimensionality Reduction: Use techniques like Principal Component Analysis (PCA) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to transform the high-dimensional data into a lower-dimensional space while preserving most of the data's variance and structure.\n",
        "\n",
        "Regularization: Apply regularization techniques, such as L1 or L2 regularization, to penalize complex models and encourage sparsity in the feature space. This helps prevent overfitting and reduces the impact of irrelevant features.\n",
        "\n",
        "Ensemble Methods: Utilize ensemble methods like Random Forests or Gradient Boosting that naturally handle high-dimensional data and feature interactions by combining multiple models.\n",
        "\n",
        "Deep Learning and Neural Networks: Deep learning architectures, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), can automatically learn relevant features from high-dimensional data, potentially eliminating the need for explicit feature engineering."
      ],
      "metadata": {
        "id": "jUKKrzHwDYtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Make a few quick notes on:\n",
        "\n",
        "PCA is an acronym for Personal Computer Analysis.\n",
        "\n",
        "2. Use of vectors\n",
        "3. Embedded technique\n",
        "\n",
        "PCA is not an acronym for Personal Computer Analysis. It stands for Principal Component Analysis, which is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving most of the data's variance.\n",
        "\n",
        "Vectors play a crucial role in many areas of mathematics and machine learning. In the context of machine learning, vectors are used to represent and manipulate data. They are often used to represent features, observations, or weights in mathematical models. Vectors have both magnitude and direction and can be operated on using various mathematical operations, such as addition, subtraction, dot product, and cross product.\n",
        "\n",
        "Embedded techniques refer to methods that incorporate feature selection or feature extraction directly within the learning algorithm. In contrast to pre-processing feature selection techniques, embedded techniques select or extract features during the model training process. Examples of embedded techniques include L1 regularization (Lasso) in linear models, which automatically performs feature selection by driving some feature coefficients to zero, and tree-based models like random forests or gradient boosting, which inherently perform feature selection through the construction of decision trees. These techniques often provide more accurate and efficient models by selecting or creating the most informative features during the learning process itself."
      ],
      "metadata": {
        "id": "6VKABFvnDYqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Make a comparison between:\n",
        "\n",
        "1. Sequential backward exclusion vs. sequential forward selection\n",
        "\n",
        "2. Function selection methods: filter vs. wrapper\n",
        "\n",
        "3. SMC vs. Jaccard coefficient\n",
        "\n",
        "Sequential Backward Exclusion vs. Sequential Forward Selection:\n",
        "\n",
        "Sequential Backward Exclusion (SBE): SBE is a feature selection method that starts with all features and iteratively removes the least significant feature at each step based on a selected criterion (e.g., statistical test, model performance). The process continues until a desired number of features or a stopping criterion is reached.\n",
        "Sequential Forward Selection (SFS): SFS begins with an empty set of features and incrementally adds the most significant feature at each step based on a chosen criterion. The process continues until a predefined number of features or a stopping criterion is met. SFS aims to find the best subset of features that maximizes the selected criterion.\n",
        "Filter vs. Wrapper Function Selection Methods:\n",
        "\n",
        "Filter Methods: Filter methods evaluate the relevance of features independently of any specific machine learning algorithm. They rank or score features based on their statistical properties (e.g., correlation, mutual information) or other domain-specific measures. Features are selected or eliminated based on these scores before applying any learning algorithm.\n",
        "Wrapper Methods: Wrapper methods involve using a specific learning algorithm as a \"wrapper\" to evaluate subsets of features. They use a search algorithm to explore different feature subsets, and the performance of the learning algorithm on each subset is used as a criterion for selection. Wrapper methods are computationally more expensive than filter methods but can capture feature interactions and provide more accurate results.\n",
        "SMC vs. Jaccard Coefficient:\n",
        "\n",
        "SMC (Similarity Matching Coefficient): SMC is a similarity measure that compares the similarity between two binary feature vectors by calculating the number of matches (similar values) divided by the total number of elements. SMC is primarily used for binary data or categorical data where the focus is on matching values.\n",
        "Jaccard Coefficient: The Jaccard coefficient is a similarity measure that calculates the size of the intersection divided by the size of the union of two sets. In the context of feature sets or binary feature vectors, the Jaccard coefficient represents the proportion of common \"1\" values (positive occurrences) between the two sets. It focuses on shared positive occurrences and does not consider negative occurrences."
      ],
      "metadata": {
        "id": "F1argId_G73_"
      }
    }
  ]
}