{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN1Ssss6UofEoI/FTBLVHN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupriyaSingh1997/ML_Assignment/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What are the key tasks involved in getting ready to work with machine learning modeling?\n",
        "\n",
        "Getting ready to work with machine learning modeling involves several key tasks. Here are the main steps involved:\n",
        "\n",
        "Defining the problem: Clearly understand the problem you are trying to solve with machine learning. Define the problem statement, objectives, and success criteria.\n",
        "\n",
        "Data collection: Gather relevant data that is representative of the problem you are trying to solve. Ensure that the data is of high quality, relevant, and sufficient for your modeling needs.\n",
        "\n",
        "Data preprocessing: Clean and preprocess the collected data to ensure its quality and prepare it for modeling. This step may involve handling missing values, outliers, data normalization, feature scaling, and handling categorical variables.\n",
        "\n",
        "Feature engineering: Extract and create relevant features from the raw data that can help improve the performance of the machine learning model. This step may involve dimensionality reduction, feature selection, or creating new features based on domain knowledge.\n",
        "\n",
        "Data splitting: Split the preprocessed data into training, validation, and testing sets. The training set is used to train the model, the validation set helps tune hyperparameters and evaluate different models, and the testing set provides an unbiased evaluation of the final model's performance.\n",
        "\n",
        "Model selection: Choose an appropriate machine learning algorithm or model that suits your problem and data characteristics. Consider factors such as interpretability, complexity, performance metrics, and the specific requirements of your problem.\n",
        "\n",
        "Model training: Train the selected model on the training data using the chosen algorithm. This involves finding the best set of model parameters or weights that minimize the chosen loss function.\n",
        "\n",
        "Model evaluation: Evaluate the trained model's performance on the validation set. Use appropriate evaluation metrics such as accuracy, precision, recall, F1 score, or area under the ROC curve to assess how well the model generalizes to unseen data.\n",
        "\n",
        "Model tuning: Fine-tune the model by adjusting hyperparameters, such as learning rate, regularization strength, or number of layers, based on the validation results. This iterative process helps improve the model's performance.\n",
        "\n",
        "Final model evaluation: Assess the final model's performance on the testing set to get an unbiased estimate of its effectiveness. This step provides an indication of how well the model is expected to perform on new, unseen data.\n",
        "\n",
        "Deployment and monitoring: Deploy the trained model into production or real-world applications. Continuously monitor the model's performance and collect feedback to ensure it remains effective over time."
      ],
      "metadata": {
        "id": "sHGaRGOqv08t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the different forms of data used in machine learning? Give a specific example for each of them.\n",
        "\n",
        "In machine learning, data can come in various forms. Here are some common types of data used in machine learning along with specific examples for each:\n",
        "\n",
        "Numerical Data: This type of data consists of numeric values and is often represented as continuous or discrete variables. Examples include:\n",
        "\n",
        "Continuous Variables: Temperature readings (e.g., 25.5°C, 30.2°C), stock prices, or sensor measurements.\n",
        "Discrete Variables: Number of customer visits, count of items purchased, or ratings on a scale of 1 to 5.\n",
        "Categorical Data: Categorical data represents qualitative variables that have distinct categories or labels. Examples include:\n",
        "\n",
        "Gender: Male, Female, Other.\n",
        "Color: Red, Blue, Green.\n",
        "Marital Status: Single, Married, Divorced.\n",
        "\n",
        "Text Data: Textual data consists of sequences of characters, words, or sentences. It is commonly used in natural language processing (NLP) tasks. Examples include:\n",
        "\n",
        "Customer reviews: \"The product is excellent!\"\n",
        "News articles: \"Stock market experiences a surge in prices.\"\n",
        "Tweets: \"Just had a delicious meal at my favorite restaurant!\"\n",
        "Image Data: Image data consists of visual information represented as pixels or matrices. Each pixel contains color or intensity values. Examples include:\n",
        "\n",
        "Handwritten digits in the MNIST dataset.\n",
        "Images of animals in a wildlife dataset.\n",
        "X-ray images for medical diagnosis.\n",
        "Audio Data: Audio data consists of sound signals captured as waveforms. Examples include:\n",
        "\n",
        "Speech recordings.\n",
        "Music tracks.\n",
        "Environmental sounds for acoustic analysis.\n",
        "Time Series Data: Time series data represents observations recorded over a continuous time interval. Examples include:\n",
        "\n",
        "Stock market prices recorded every minute.\n",
        "Temperature readings over a day.\n",
        "\n",
        "Energy consumption data over a month.\n",
        "Structured Data: Structured data is organized in a tabular format with rows and columns. It is commonly stored in databases or spreadsheets. Examples include:\n",
        "\n",
        "Customer data: ID, age, income, and purchase history.\n",
        "Financial data: Date, transaction type, amount, and category."
      ],
      "metadata": {
        "id": "rjRVKnVYv05I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Distinguish:\n",
        "\n",
        "1. Numeric vs. categorical attributes\n",
        "\n",
        "2. Feature selection vs. dimensionality reduction\n",
        "\n",
        "Numeric vs. Categorical Attributes:\n",
        "\n",
        "Numeric attributes: Numeric attributes represent continuous or discrete numerical values. They can be further categorized into continuous or interval variables. Examples of numeric attributes include temperature, age, or height. Numeric attributes allow for mathematical operations such as addition, subtraction, and averaging.\n",
        "\n",
        "Categorical attributes: Categorical attributes represent qualitative variables with distinct categories or labels. They do not possess a numerical value that can be used for mathematical operations. Examples of categorical attributes include gender, color, or marital status. Categorical attributes can be further divided into nominal variables (categories with no inherent order) or ordinal variables (categories with a specific order).\n",
        "\n",
        "The distinction between numeric and categorical attributes is important because it determines the type of statistical analysis and machine learning algorithms that can be applied to the data. Numeric attributes are typically used in regression models, while categorical attributes often require encoding techniques (such as one-hot encoding) to be used effectively in machine learning algorithms.\n",
        "\n",
        "Feature Selection vs. Dimensionality Reduction:\n",
        "\n",
        "Feature selection: Feature selection is the process of selecting a subset of relevant features from the original set of features. It aims to identify the most informative and discriminative features that contribute significantly to the prediction task. By reducing the number of features, feature selection helps improve model performance, reduces overfitting, and enhances interpretability. Common feature selection techniques include statistical tests, correlation analysis, and recursive feature elimination.\n",
        "\n",
        "Dimensionality reduction: Dimensionality reduction is the process of transforming high-dimensional data into a lower-dimensional representation while preserving its essential characteristics. It aims to overcome the curse of dimensionality, reduce computational complexity, and remove redundant or irrelevant features. Dimensionality reduction techniques can be divided into two main categories:\n",
        "\n",
        "Feature extraction: These techniques aim to project the data onto a lower-dimensional space by creating new derived features that capture most of the original information. Examples include Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).\n",
        "\n",
        "Feature projection: These techniques aim to find a linear or nonlinear transformation that preserves the pairwise distances between data points. Examples include t-Distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP)."
      ],
      "metadata": {
        "id": "E4WzZfw6v025"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Make quick notes on any two of the following:\n",
        "\n",
        "1. The histogram\n",
        "\n",
        "2. Use a scatter plot\n",
        "\n",
        "3.PCA (Personal Computer Aid)\n",
        "\n",
        "1. The Histogram:\n",
        "\n",
        "A histogram is a graphical representation of the distribution of a dataset. It is used to visualize the frequency or count of values falling within different intervals, known as bins.\n",
        "The x-axis represents the range of values in the dataset, divided into bins, while the y-axis represents the frequency or count of values in each bin.\n",
        "Histograms are commonly used to understand the distribution of numeric data, identify outliers, and detect patterns or anomalies.\n",
        "They can provide insights into the central tendency (mean, median) and spread (variance, standard deviation) of the data.\n",
        "Histograms can be visually interpreted to identify if the data is normally distributed, skewed, or exhibits other distribution shapes.\n",
        "\n",
        "2. Scatter Plot:\n",
        "\n",
        "A scatter plot is a two-dimensional plot that displays the relationship between two variables or attributes.\n",
        "Each data point in the plot represents the value of one variable on the x-axis and another variable on the y-axis.\n",
        "Scatter plots are useful for visualizing patterns, trends, and correlations between variables.\n",
        "They can reveal the presence of clusters, outliers, or nonlinear relationships between variables.\n",
        "Scatter plots are often used in exploratory data analysis (EDA) to gain insights into the relationship between variables before applying more advanced statistical or machine learning techniques.\n",
        "They are particularly helpful in identifying associations and understanding the strength and direction of relationships between variables.\n",
        "3. PCA (Principal Component Analysis):\n",
        "\n",
        "PCA stands for Principal Component Analysis and is a dimensionality reduction technique.\n",
        "It is used to transform high-dimensional data into a lower-dimensional space while preserving most of the original information.\n",
        "PCA identifies the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
        "The first principal component explains the largest amount of variance, followed by the second, and so on.\n",
        "PCA can be used for various purposes, such as data visualization, noise reduction, feature extraction, and speeding up machine learning algorithms.\n",
        "It helps to identify the most important features that contribute to the variability in the data and reduce the impact of less informative or redundant features.\n",
        "PCA is especially useful when dealing with high-dimensional datasets where the number of features is large compared to the number of observations."
      ],
      "metadata": {
        "id": "0eJJeJ77v004"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Why is it necessary to investigate data? Is there a discrepancy in how qualitative and quantitative data are explored?\n",
        "\n",
        "Investigating data is crucial in order to gain insights, understand patterns, and make informed decisions. It allows us to uncover valuable information, identify trends, detect outliers, and validate assumptions. Both qualitative and quantitative data require exploration, but there can be some differences in how they are explored:\n",
        "\n",
        "1. Investigating Quantitative Data:\n",
        "\n",
        "Quantitative data consists of numerical values and is often analyzed using statistical methods.\n",
        "Key reasons for investigating quantitative data include:\n",
        "Descriptive statistics: Exploring measures of central tendency (mean, median) and dispersion (variance, standard deviation) to understand the distribution of data.\n",
        "Detecting outliers: Identifying extreme values that deviate significantly from the bulk of the data.\n",
        "\n",
        "Checking assumptions: Assessing the validity of assumptions required for statistical analyses, such as normality, independence, or homoscedasticity.\n",
        "Hypothesis testing: Conducting statistical tests to determine if there are significant differences or relationships between variables.\n",
        "Correlation and regression analysis: Examining the strength and direction of relationships between variables.\n",
        "2. Investigating Qualitative Data:\n",
        "\n",
        "Qualitative data consists of non-numeric information such as observations, interviews, or open-ended survey responses.\n",
        "Key reasons for investigating qualitative data include:\n",
        "Thematic analysis: Identifying common themes, patterns, or recurring ideas within the qualitative data.\n",
        "Coding and categorization: Assigning codes or labels to segments of data to create meaningful categories.\n",
        "Contextual understanding: Analyzing the context and circumstances surrounding the qualitative data to gain deeper insights.\n",
        "\n",
        "Interpretation and meaning-making: Drawing inferences and interpretations based on the qualitative data to generate new knowledge.\n",
        "Qualitative data exploration often involves a more subjective and interpretive approach, relying on researcher expertise and engagement with the data.\n",
        "While the exploration of quantitative data often involves statistical measures and formal tests, exploring qualitative data is typically more flexible, iterative, and reliant on qualitative research methods such as content analysis or thematic analysis. However, both types of data exploration aim to uncover insights and support decision-making, albeit with different analytical techniques."
      ],
      "metadata": {
        "id": "G895sC5Uv0zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are the various histogram shapes? What exactly are ‘bins&#39;?\n",
        "\n",
        "Histograms can exhibit different shapes, providing insights into the underlying distribution of the data. Here are some common histogram shapes:\n",
        "\n",
        "Normal (Gaussian) Distribution: The histogram forms a bell-shaped curve with a symmetric distribution around the mean. It indicates that the data is centered and follows a normal distribution.\n",
        "\n",
        "Skewed Distribution:\n",
        "\n",
        "Positive Skew (Right Skew): The histogram has a long tail on the right side, indicating that the majority of the data is concentrated towards the left. The mean is greater than the median.\n",
        "Negative Skew (Left Skew): The histogram has a long tail on the left side, indicating that the majority of the data is concentrated towards the right. The mean is less than the median.\n",
        "Bimodal Distribution: The histogram exhibits two distinct peaks, suggesting the presence of two different groups or populations in the data.\n",
        "\n",
        "Uniform Distribution: The histogram displays a relatively flat and constant distribution, indicating that the data is evenly spread across the range of values.\n",
        "\n",
        "Multimodal Distribution: The histogram shows multiple peaks, indicating the presence of multiple distinct groups or populations in the data.\n",
        "\n",
        "Bins in a histogram refer to the intervals or ranges into which the data is divided. The x-axis of a histogram represents the range of values, which is divided into several bins or intervals. Each bin represents a specific range of values, and the height or frequency of the histogram bars corresponds to the number of data points falling within each bin.\n",
        "\n",
        "The number of bins in a histogram can affect the shape and interpretability of the distribution. A small number of bins may result in a loss of detail, while a large number of bins can lead to noise or sparsity. The choice of the number of bins depends on the characteristics of the data and the level of detail required for analysis. Common methods for selecting the number of bins include the square root choice, Sturges' formula, or using domain knowledge and experimentation."
      ],
      "metadata": {
        "id": "AoPNN0tHv0xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do we deal with data outliers?\n",
        "\n",
        "Dealing with data outliers is an important step in data preprocessing to ensure the accuracy and reliability of the analysis. Outliers are data points that deviate significantly from the majority of the data and can have a significant impact on statistical measures and machine learning models. Here are some common approaches to handle data outliers:\n",
        "\n",
        "Identifying outliers: Before dealing with outliers, it is essential to identify them. This can be done using statistical methods such as z-scores, standard deviations, box plots, or visual inspection of scatter plots. Outliers are observations that fall beyond a certain threshold or exhibit extreme values compared to the rest of the data.\n",
        "\n",
        "Understanding the context: It is crucial to understand the nature and context of the data before deciding how to handle outliers. Outliers can be genuine data points that represent rare events, anomalies, or important information. In some cases, outliers may provide valuable insights and should not be removed without careful consideration.\n",
        "\n",
        "Data transformation: Data transformation techniques can be applied to reduce the impact of outliers. Common transformations include logarithmic, square root, or reciprocal transformations. These transformations can compress the range of values and reduce the influence of extreme observations.\n",
        "\n",
        "Winsorization: Winsorization involves replacing outliers with less extreme values. The extreme values are replaced with either the minimum or maximum value within a predetermined range. Winsorization can help mitigate the impact of outliers while preserving the overall distribution of the data.\n",
        "\n",
        "Removing outliers: In certain cases, outliers can be removed from the dataset. However, caution should be exercised as removing outliers can significantly affect the characteristics of the data and may introduce bias. Outliers should only be removed after careful consideration and if they are confirmed as data entry errors or measurement errors.\n",
        "\n",
        "Robust statistical techniques: Using robust statistical techniques that are less sensitive to outliers can be an effective approach. These techniques, such as robust regression or robust estimators, give less weight to extreme observations and are more resistant to the influence of outliers.\n",
        "\n",
        "Modeling techniques: Some machine learning algorithms are inherently robust to outliers, such as tree-based algorithms or support vector machines. These models can handle outliers to some extent by partitioning or finding optimal decision boundaries.\n",
        "\n",
        "When dealing with outliers, it is important to document the chosen approach and rationale behind it. It is also advisable to compare and analyze the data with and without outliers to assess the impact on the results and make informed decisions accordingly."
      ],
      "metadata": {
        "id": "QayIkChxv0vA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Describe how a scatter plot can be used to investigate bivariate relationships. Is it possible to find outliers using a scatter plot?\n",
        "\n",
        "A scatter plot is a powerful visualization tool that allows us to investigate the relationship between two variables in a bivariate dataset. It plots the values of one variable on the x-axis and the values of another variable on the y-axis, with each data point representing a pair of values from the dataset. Here's how a scatter plot can be used to investigate bivariate relationships:\n",
        "\n",
        "Identifying Patterns: By examining the scatter plot, we can identify patterns or trends in the relationship between the two variables. This can include positive or negative associations, linear or nonlinear relationships, or no apparent relationship.\n",
        "\n",
        "Correlation Assessment: The scatter plot can help assess the strength and direction of the correlation between the two variables. If the data points form a distinct pattern or follow a trend, it suggests a correlation between the variables. The slope, direction, and density of the points provide visual cues about the relationship.\n",
        "\n",
        "Outlier Detection: Scatter plots can be useful in identifying outliers. Outliers are data points that deviate significantly from the bulk of the data. They can be observed as data points that fall far away from the general trend or pattern of the other data points in the plot. Outliers can be indicative of errors, anomalies, or extreme observations that require further investigation.\n",
        "\n",
        "Grouping and Clustering: Scatter plots can reveal grouping or clustering behavior in the data. If distinct clusters or groups are visible, it suggests that the two variables have different behaviors or patterns within each cluster.\n",
        "\n",
        "Assessing Homoscedasticity: Scatter plots can help assess the homoscedasticity (constant variance) assumption in regression analysis. If the spread of data points remains consistent as the values of the independent variable change, it suggests homoscedasticity. If the spread varies as the values change, it indicates heteroscedasticity.\n",
        "\n",
        "While scatter plots can provide valuable insights into bivariate relationships, they may not always reveal outliers in a straightforward manner. Outliers can be identified as data points that deviate significantly from the general pattern, but determining whether a data point is an outlier often requires additional statistical analysis or thresholding based on domain knowledge. Nevertheless, examining the scatter plot can provide visual cues that prompt further investigation into potential outliers."
      ],
      "metadata": {
        "id": "xeXyXnB-v0s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Describe how cross-tabs can be used to figure out how two variables are related.\n",
        "\n",
        "Cross-tabulation, also known as a contingency table or crosstab, is a tabular method used to analyze the relationship between two categorical variables. It provides a summary of the distribution of data across the categories of both variables. Here's how cross-tabs can be used to figure out how two variables are related:\n",
        "\n",
        "Creating the Cross-Tabulation: To create a cross-tabulation, the data is organized into rows and columns, with one variable's categories forming the rows and the other variable's categories forming the columns. Each cell in the table represents the count or frequency of observations that fall into a specific combination of categories.\n",
        "\n",
        "Examining the Marginal Distributions: Cross-tabs provide the marginal distributions, which are the totals for each category of the two variables separately. This allows you to understand the distribution of each variable individually. You can analyze the frequency or percentage of observations within each category to gain insights into the distribution patterns.\n",
        "Assessing Association or Dependence: Cross-tabs help assess the association or dependence between the two variables. By examining the cell frequencies or percentages, you can identify if there is a relationship or pattern between the variables. For example, if the counts or proportions are not evenly distributed across the cells, it indicates a potential relationship.\n",
        "\n",
        "Calculating Conditional Distributions: Cross-tabs enable the calculation of conditional distributions, which reveal the distribution of one variable within the categories of the other variable. By examining the proportions or percentages within each column or row, you can understand how the distribution of one variable varies based on the categories of the other variable.\n",
        "\n",
        "Hypothesis Testing: Cross-tabs can be used for hypothesis testing to determine if the association between the two variables is statistically significant. Statistical tests such as the chi-square test can be applied to evaluate the null hypothesis that there is no association between the variables.\n",
        "\n",
        "Visualization: Cross-tabs can be visually represented using heatmaps or stacked bar charts to provide a clear visual depiction of the relationship between the variables."
      ],
      "metadata": {
        "id": "AYGVz0J9v0qv"
      }
    }
  ]
}