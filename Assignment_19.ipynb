{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlnIC7CLskOQZBc2LmZlwY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupriyaSingh1997/ML_Assignment/blob/main/Assignment_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
        "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
        "b) For each set of centroid values, calculate the SSE.\n",
        "\n",
        "To create clusters using the k-means method and calculate the Sum of Squared Errors (SSE) for each set of centroids, we'll follow these steps:\n",
        "\n",
        "a) K-means clustering:\n",
        "\n",
        "Step 1: Initialize the centroids for each cluster.\n",
        "\n",
        "For the first set of centroids: 15, 32\n",
        "For the second set of centroids: 12, 30\n",
        "Step 2: Assign data points to the nearest centroid.\n",
        "\n",
        "Calculate the distance between each data point and the centroids.\n",
        "Assign each data point to the nearest centroid based on the minimum distance.\n",
        "Using the first set of centroids (15, 32):\n",
        "Data points: 5, 10, 15, 20, 25, 30, 35\n",
        "\n",
        "Data Point | Distance to 15 | Distance to 32 | Assigned Cluster\n",
        "5 | 10 | 27 | Cluster 1\n",
        "10 | 5 | 22 | Cluster 1\n",
        "15 | 0 | 17 | Cluster 1\n",
        "20 | 5 | 12 | Cluster 1\n",
        "25 | 10 | 7 | Cluster 1\n",
        "30 | 15 | 2 | Cluster 1\n",
        "35 | 20 | 3 | Cluster 2\n",
        "\n",
        "Using the second set of centroids (12, 30):\n",
        "Data points: 5, 10, 15, 20, 25, 30, 35\n",
        "\n",
        "Data Point | Distance to 12 | Distance to 30 | Assigned\n",
        "Cluster\n",
        "5 | 7 | 5 | Cluster 1\n",
        "10 | 2 | 20 | Cluster 1\n",
        "15 | 3 | 15 | Cluster 1\n",
        "20 | 8 | 10 | Cluster 2\n",
        "25 | 13 | 5 | Cluster 2\n",
        "30 | 18 | 0 | Cluster 2\n",
        "35 | 23 | 5 | Cluster 2\n",
        "\n",
        "b) Calculation of SSE:\n",
        "\n",
        "SSE is the sum of the squared distances between each data point and its assigned centroid. Here's how to calculate it:\n",
        "\n",
        "Using the first set of centroids (15, 32):\n",
        "\n",
        "Cluster 1: (5, 10, 15, 20, 25, 30)\n",
        "\n",
        "SSE = (0^2 + 5^2 + 10^2 + 5^2 + 10^2 + 15^2) = 355\n",
        "\n",
        "Cluster 2: (35)\n",
        "\n",
        "SSE = (20^2) = 400\n",
        "Total SSE = SSE_cluster1 + SSE_cluster2 = 355 + 400 = 755\n",
        "\n",
        "Using the second set of centroids (12, 30):\n",
        "\n",
        "Cluster 1: (5, 10, 15)\n",
        "\n",
        "SSE = (7^2 + 2^2 + 3^2) = 62\n",
        "Cluster 2: (20, 25, 30, 35)\n",
        "\n",
        "SSE = (8^2 + 13^2 + 18^2 + 23^2) = 964\n",
        "Total SSE = SSE_cluster1 + SSE_cluster2 = 62 + 964 = 1026\n",
        "\n",
        "Therefore, for the given set of one-dimensional data points and the two sets of centroids, the SSE values are as follows:\n",
        "\n",
        "For the first set of centroids: SSE = 755\n",
        "For the second set of centroids: SSE = 1026"
      ],
      "metadata": {
        "id": "HfFZJWw7uOn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
        "\n",
        "Market Basket Analysis (also known as Association Analysis or Affinity Analysis) is a data mining technique that explores relationships and patterns in transactional data to identify associations between items frequently purchased together. It makes use of association analysis concepts to uncover valuable insights about customer behavior and facilitate decision-making in areas such as product placement, cross-selling, and targeted marketing strategies. Here's how Market Basket Analysis leverages association analysis concepts:\n",
        "\n",
        "Association Rule: At the core of Market Basket Analysis is the concept of association rules. An association rule consists of an antecedent (the items that are already present in the transaction) and a consequent (the item that is likely to be purchased alongside the antecedent). For example, \"If a customer buys milk and bread, then they are likely to buy eggs.\" These rules are discovered based on the frequency and co-occurrence of items in transactions.\n",
        "\n",
        "Support: Support is a measure that indicates the frequency or occurrence of a particular item or itemset in the dataset. In the context of Market Basket Analysis, it represents the proportion of transactions that contain a specific item or itemset. It helps identify items that are frequently purchased together and form the basis for generating association rules.\n",
        "\n",
        "Confidence: Confidence is a measure that quantifies the likelihood of the consequent item being purchased when the antecedent item(s) are already present in the transaction. It is calculated as the ratio of the support for both the antecedent and consequent items to the support of the antecedent. High confidence values indicate a strong association between items.\n",
        "\n",
        "Lift: Lift is a measure that determines the strength of the association between the antecedent and consequent items, taking into account the expected co-occurrence based on individual item frequencies. It is calculated as the ratio of the confidence of the rule to the expected confidence if the antecedent and consequent were independent. Lift values greater than 1 indicate a positive association, while values less than 1 indicate a negative association.\n",
        "\n",
        "Itemset: An itemset refers to a collection of items that occur together in a transaction. In Market Basket Analysis, itemsets can be of different sizes, ranging from single items (1-itemset) to multiple items (2-itemset, 3-itemset, etc.). Discovering frequent itemsets helps identify common patterns and associations among items."
      ],
      "metadata": {
        "id": "6ngsI8n2uOkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Give an example of the Apriori algorithm for learning association rules.\n",
        "\n",
        "Suppose we have a transactional dataset that represents customer purchases in a grocery store. The dataset contains the following transactions:\n",
        "\n",
        "Transaction 1: Bread, Milk\n",
        "Transaction 2: Bread, Diapers, Beer, Eggs\n",
        "Transaction 3: Milk, Diapers, Beer, Coke\n",
        "Transaction 4: Bread, Milk, Diapers, Beer\n",
        "Transaction 5: Bread, Milk, Diapers, Coke\n",
        "\n",
        "Step 1: Generating frequent 1-itemsets:\n",
        "To start, we scan the dataset and count the occurrences of each item.\n",
        "\n",
        "Item | Count\n",
        "Bread | 4\n",
        "Milk | 4\n",
        "Diapers | 4\n",
        "Beer | 3\n",
        "Coke | 2\n",
        "Eggs | 1\n",
        "\n",
        "We only consider items with a minimum support threshold (e.g., support > 2).\n",
        "\n",
        "Frequent 1-itemsets:\n",
        "{Bread}, {Milk}, {Diapers}, {Beer}, {Coke}\n",
        "\n",
        "Step 2: Generating frequent 2-itemsets:\n",
        "Using the frequent 1-itemsets, we generate possible 2-itemsets and count their occurrences in the dataset.\n",
        "\n",
        "Possible 2-itemsets:\n",
        "{Bread, Milk}, {Bread, Diapers}, {Bread, Beer}, {Bread, Coke}\n",
        "{Milk, Diapers}, {Milk, Beer}, {Milk, Coke}\n",
        "{Diapers, Beer}, {Diapers, Coke}\n",
        "\n",
        "Counting occurrences:\n",
        "\n",
        "Itemset | Count\n",
        "{Bread, Milk} | 3\n",
        "{Bread, Diapers} | 3\n",
        "{Bread, Beer} | 2\n",
        "{Bread, Coke} | 1\n",
        "{Milk, Diapers} | 3\n",
        "{Milk, Beer} | 2\n",
        "{Milk, Coke} | 1\n",
        "{Diapers, Beer} | 3\n",
        "{Diapers, Coke} | 2\n",
        "\n",
        "We only consider itemsets with a minimum support threshold (e.g., support > 2).\n",
        "Frequent 2-itemsets:\n",
        "{Bread, Milk}, {Bread, Diapers}, {Milk, Diapers}, {Diapers, Beer}\n",
        "\n",
        "Step 3: Generating frequent 3-itemsets:\n",
        "Using the frequent 2-itemsets, we generate possible 3-itemsets and count their occurrences in the dataset.\n",
        "\n",
        "Possible 3-itemsets:\n",
        "{Bread, Milk, Diapers}, {Bread, Milk, Beer}, {Bread, Diapers, Beer}, {Milk, Diapers, Beer}\n",
        "\n",
        "Counting occurrences:\n",
        "\n",
        "Itemset | Count\n",
        "{Bread, Milk, Diapers} | 2\n",
        "{Bread, Milk, Beer} | 1\n",
        "{Bread, Diapers, Beer} | 2\n",
        "{Milk, Diapers, Beer} | 2\n",
        "We only consider itemsets with a minimum support threshold (e.g., support > 2).\n",
        "\n",
        "Frequent 3-itemsets:\n",
        "{Bread, Milk, Diapers}, {Bread, Diapers, Beer}, {Milk, Diapers, Beer}\n",
        "\n",
        "Step 4: Generating association rules:\n",
        "With the frequent itemsets, we can now generate association rules based on a minimum confidence threshold (e.g., confidence > 0.5).\n",
        "\n",
        "For example, let's consider the rule: {Bread, Milk} => {Diapers}\n",
        "\n",
        "Support({Bread, Milk, Diapers}) = 2\n",
        "Support({Bread, Milk}) = 3\n",
        "\n",
        "Confidence({Bread, Milk} => {Diapers}) = Support({Bread, Milk, Diapers}) / Support({Bread, Milk}) = 2/3\n",
        "\n",
        "If the confidence exceeds the minimum threshold, we consider it a valid rule.\n",
        "In this example, all the frequent 2-itemsets and 3-itemsets can be used to generate association rules based on their confidence values.\n",
        "\n",
        "The Apriori algorithm iteratively continues these steps, generating larger itemsets until no more frequent itemsets can be found or until a specific maximum itemset size is reached."
      ],
      "metadata": {
        "id": "kMNw6l7KuOh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration.\n",
        "\n",
        "In hierarchical clustering, the distance between clusters is measured using different metrics depending on the specific method chosen. Two common distance metrics used in hierarchical clustering are:\n",
        "\n",
        "Single-linkage (or minimum distance): The distance between two clusters is defined as the shortest distance between any two points, one from each cluster.\n",
        "\n",
        "Complete-linkage (or maximum distance): The distance between two clusters is defined as the maximum distance between any two points, one from each cluster.\n",
        "\n",
        "Other distance metrics like average linkage, centroid linkage, and Ward's method can also be used.\n",
        "\n",
        "The decision of when to end the iteration and stop merging clusters is typically based on a predetermined stopping criterion. One common approach is to use a threshold distance or a predefined number of clusters.\n",
        "\n",
        "Here's how the stopping criterion is used:\n",
        "\n",
        "Threshold distance: A threshold distance is set, and the clustering process continues until the distance between the closest pair of clusters exceeds this threshold. This ensures that clusters are not merged if their distance is greater than the specified value.\n",
        "\n",
        "Predefined number of clusters: Instead of using a threshold distance, a fixed number of clusters is predetermined. The clustering process continues until the desired number of clusters is reached. This approach allows for a specific level of granularity in the resulting clusters.\n",
        "\n",
        "Once the stopping criterion is met, the clustering process stops, and the hierarchy of clusters is obtained. The result is often visualized using a dendrogram, which represents the merging process and displays the distances between clusters."
      ],
      "metadata": {
        "id": "rbXAJycduOfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
        "\n",
        "In the k-means algorithm, the process of recomputing the cluster centroids involves updating the positions of the centroids based on the current assignment of data points to clusters. Here's a step-by-step explanation of how the cluster centroids are recomputed:\n",
        "\n",
        "Assign data points to clusters: In the k-means algorithm, each data point is initially assigned to a cluster based on the proximity to the centroid. This assignment is determined by calculating the distance between each data point and the centroids and assigning the point to the nearest centroid.\n",
        "\n",
        "Recompute centroids: After the initial assignment, the centroids' positions are updated by calculating the mean (average) of the data points assigned to each cluster. The mean is calculated separately for each dimension of the data points.\n",
        "\n",
        "Update centroid positions: Each centroid's position is updated to the new mean position calculated in the previous step. This results in a new set of centroid coordinates for each cluster.\n",
        "\n",
        "Repeat steps 1-3: Steps 1 to 3 are repeated iteratively until convergence. In each iteration, the data points are reassigned to the nearest centroid, and the centroids are recomputed based on the updated cluster assignments. The process continues until the algorithm converges, which means that the centroid positions stabilize and do not change significantly between iterations or until a maximum number of iterations is reached."
      ],
      "metadata": {
        "id": "CJjpmyH9uOdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. At the start of the clustering exercise, discuss one method for determining the required number of clusters.\n",
        "\n",
        "Determining the required number of clusters, also known as finding the optimal number of clusters, is an important step in clustering analysis. While there isn't a definitive method that guarantees the \"correct\" number of clusters, several techniques can provide insights and help make an informed decision. One commonly used method is the Elbow Method.\n",
        "\n",
        "The Elbow Method involves evaluating the sum of squared distances (SSE) between data points and their respective cluster centroids for different numbers of clusters. The idea is to observe the SSE as a function of the number of clusters and identify a point on the plot where the decrease in SSE begins to level off.\n",
        "\n",
        "Here's how the Elbow Method works:\n",
        "Perform clustering: Apply the k-means algorithm (or any other clustering algorithm) to the dataset for different numbers of clusters, ranging from a minimum to a maximum value.\n",
        "\n",
        "Compute SSE: For each clustering iteration, calculate the sum of squared distances (SSE) between each data point and its assigned cluster centroid. SSE represents the overall compactness of the clusters.\n",
        "\n",
        "Plot SSE versus number of clusters: Create a line plot or a scatter plot with the number of clusters on the x-axis and the corresponding SSE on the y-axis. Each point on the plot represents an SSE value for a specific number of clusters.\n",
        "\n",
        "Identify the \"elbow\" point: Examine the plot and look for a point where the SSE decreases significantly with each increase in the number of clusters and then levels off. This point resembles an elbow shape in the plot. The idea is to find the number of clusters at this elbow point, as it indicates a significant improvement in clustering performance without a significant increase in complexity.\n",
        "\n",
        "It's important to note that the elbow point is not always clear-cut, and there can be some subjectivity in determining the optimal number of clusters. Sometimes, the plot may not exhibit a distinct elbow, making the decision more challenging. In such cases, it may be helpful to consider other evaluation metrics, such as silhouette score, or domain knowledge to guide the choice of the number of clusters."
      ],
      "metadata": {
        "id": "AWNn3j_2uOab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
        "\n",
        "The k-means algorithm has several advantages and disadvantages. Let's explore them:\n",
        "\n",
        "Advantages of the k-means algorithm:\n",
        "\n",
        "Simplicity: The k-means algorithm is relatively simple and easy to understand compared to more complex clustering algorithms. It follows an iterative process of assigning data points to clusters and updating centroids, making it straightforward to implement.\n",
        "\n",
        "Efficiency: K-means is computationally efficient, especially for large datasets, as it scales well with the number of data points. The algorithm's time complexity is typically linear with respect to the number of data points.\n",
        "\n",
        "Scalability: K-means can handle large datasets with a high number of dimensions efficiently. It works well with numeric data and can be applied to a wide range of problems, making it versatile.\n",
        "\n",
        "Interpretability: The resulting clusters in k-means can often be interpreted and easily understood. The algorithm produces clusters with well-defined centroid points, allowing for straightforward analysis and interpretation.\n",
        "\n",
        "Disadvantages of the k-means algorithm:\n",
        "Sensitive to initial centroids: The k-means algorithm's final clustering result can vary depending on the initial placement of the centroids. Different initializations can lead to different outcomes, making it important to run the algorithm multiple times with different initializations or use techniques like k-means++ to mitigate this issue.\n",
        "\n",
        "Requires pre-defined k value: The k-means algorithm requires the number of clusters (k) to be specified in advance. Determining the optimal number of clusters can be challenging, and choosing an inappropriate k value may result in suboptimal clustering.\n",
        "\n",
        "Sensitive to outliers: K-means is sensitive to outliers as they can significantly affect the position of the centroids. Outliers may distort the clusters and lead to suboptimal results.\n",
        "\n",
        "Assumes spherical clusters: K-means assumes that the clusters have a spherical shape and equal variance. It may struggle to handle clusters with complex shapes, different sizes, or varying densities. In such cases, other clustering algorithms like DBSCAN or Gaussian Mixture Models may be more appropriate.\n",
        "\n",
        "Limited to numerical data: K-means is designed for numerical data and may not be suitable for categorical or binary data without appropriate preprocessing techniques.\n",
        "\n",
        "May converge to local optima: K-means can converge to local optima rather than the global optimum. The outcome of the algorithm depends on the initial placement of centroids, and it may not always find the best possible clustering solution."
      ],
      "metadata": {
        "id": "3TcC0XSPuOXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Draw a diagram to demonstrate the principle of clustering.\n",
        "\n",
        "As a text-based AI, I'm unable to directly generate diagrams or images. However, I can describe a diagram illustrating the principle of clustering:\n",
        "\n",
        "Imagine a scatter plot with data points distributed in two-dimensional space. Each data point represents an observation or instance. The x-axis represents one feature or attribute, while the y-axis represents another feature.\n",
        "\n",
        "In the scatter plot, clusters are formed by grouping data points that are close together. A cluster is a collection of points that share similarity or proximity based on certain criteria or distance measures. The goal of clustering is to partition the data points into clusters in a way that maximizes the intra-cluster similarity and minimizes the inter-cluster similarity.\n",
        "\n",
        "The diagram would show distinct groups of data points clustered closely together, with some separation or distance between the clusters. Each cluster would be represented by a different color or symbol to visually distinguish them.The clustering process aims to find these natural groupings or patterns within the data, helping to reveal underlying structures, similarities, or relationships among the observations. Clustering can aid in data exploration, pattern recognition, anomaly detection, and various other data analysis tasks."
      ],
      "metadata": {
        "id": "0Pehy0lJuOU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
        "C2, and C3 have the following findings after the first iteration:\n",
        "\n",
        "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
        "\n",
        "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
        "\n",
        "C3: (5,5) and (9,9)\n",
        "\n",
        "What would the cluster centroids be if you were to run a second iteration? What would this\n",
        "clustering SSE be?\n",
        "\n",
        "To determine the cluster centroids after a second iteration and calculate the Sum of Squared Errors (SSE) for the given clustering, let's review the information provided:\n",
        "\n",
        "Initial Clusters:\n",
        "C1: (2,2), (4,4), (6,6)\n",
        "C2: (2,2), (4,4), (6,6)\n",
        "C3: (2,2), (4,4)\n",
        "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
        "C3: (5,5) and (9,9)\n",
        "\n",
        "Based on the provided data points, it seems that there is an incomplete or missing information for Cluster C2, and there is a typo in the description. However, I will assume the intended data points for the clusters are as follows:\n",
        "\n",
        "After the first iteration, the clusters become:\n",
        "\n",
        "C1: (2,2), (4,4), (6,6)\n",
        "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4)\n",
        "C3: (5,5), (9,9)\n",
        "\n",
        "To calculate the centroids, we take the mean of the coordinates in each cluster:\n",
        "\n",
        "C1 centroid: (2+4+6)/3, (2+4+6)/3 = (4,4)\n",
        "C2 centroid: (0+4+0+0+0+0+0+0+0)/9, (4+0+4+4+4+4+4+4+4)/9 = (0.44, 4)\n",
        "C3 centroid: (5+9)/2, (5+9)/2 = (7, 7)\n",
        "\n",
        "Now, assuming the intended data points for Cluster C2 were (0,4) repeated nine times, we have the corrected data points:\n",
        "\n",
        "C2: (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4)\n",
        "C2: (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4)\n",
        "\n",
        "After the second iteration, the updated clusters become:\n",
        "\n",
        "C1: (2,2), (4,4), (6,6)\n",
        "C2: (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4)\n",
        "C3: (5,5), (9,9)\n",
        "\n",
        "The updated cluster centroids after the second iteration are:\n",
        "\n",
        "C1 centroid: (2+4+6)/3, (2+4+6)/3 = (4,4)\n",
        "C2 centroid: (0+0+0+0+0+0+0+0+0)/9, (4+4+4+4+4+4+4+4+4)/9 = (0, 4)\n",
        "C3 centroid: (5+9)/2, (5+9)/2 = (7, 7)"
      ],
      "metadata": {
        "id": "l47pc8pBwIhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
        "be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm.\n",
        "\n",
        "As a text-based AI, I'm unable to directly generate diagrams or images. However, I can describe a diagram illustrating the process of clustering defects in a software project using the k-means algorithm:\n",
        "\n",
        "Start with 20 defect data points: Imagine a set of 20 defect data points represented as dots or circles in a diagram.\n",
        "\n",
        "Apply the k-means algorithm: Use the k-means algorithm to cluster these defect data points into five clusters. Each cluster represents a group of related defects based on their text analytics or similarity in defect details.\n",
        "\n",
        "Cluster assignment: Assign each defect data point to one of the five clusters based on its proximity to the cluster's centroid. In the diagram, you can show different colored circles or symbols representing each cluster, and the defect data points are assigned to the nearest cluster.\n",
        "\n",
        "New defect classification: After the initial clustering is completed, any new defect that arises must be classified into one of the existing five clusters. When a new defect is discovered, its defect details are analyzed using the same text analytics approach. Then, based on the similarity to the existing clusters, it is assigned to the appropriate cluster.\n",
        "\n",
        "The diagram would show the original 20 defect data points divided into five distinct clusters. Each cluster would be visually represented by a different color or symbol, such as circles or clusters with unique identifiers.\n",
        "\n",
        "The benefit of this process is that by clustering the defects, the team can identify common patterns or types of software flaws. When a new defect emerges, it can be quickly classified into one of the existing clusters, enabling better organization and identification of similar issues."
      ],
      "metadata": {
        "id": "wyMitTblwIeK"
      }
    }
  ]
}